# Basic Matrix Library  
  ### Focused on Fast Matrix Multiplication (1 Non-SIMD Core Constraint)

  This project explores progressively faster algorithms for square matrix multiplication under the constraint of a single non-SIMD core. The improvements come entirely from better algorithmic design and cache efficiency.

  ---

  ## Naive Multiplication ‚Äì ùëÇ(n¬≥)

  | Matrix Size            | Time Elapsed |
  |------------------------|--------------|
  | 32√ó32 √ó 32√ó32          |      0 ms    |
  | 128√ó128 √ó 128√ó128      |     27 ms    |
  | 256√ó256 √ó 256√ó256      |    222 ms    |
  | 512√ó512 √ó 512√ó512      |   1770 ms    |
  | 1024√ó1024 √ó 1024√ó1024   |  16887 ms    |

  ---

  ## Divide and Conquer ‚Äì Still ùëÇ(n¬≥), But Faster

  Uses a recursive approach that applies naive multiplication at small sizes (block size = 64). This allows better use of cache:
  
  - Smaller matrices are more likely to fit in L1/L2/L3 cache.
  - Columns of `m2` are reused often and fetched from cache.
  - Conceptually similar to blocked/tiling approaches, with some overhead.
  
  | Matrix Size            | Time Elapsed |
  |------------------------|--------------|
  | 32√ó32 √ó 32√ó32          |      0 ms    |
  | 128√ó128 √ó 128√ó128      |     19 ms    |
  | 256√ó256 √ó 256√ó256      |    163 ms    |
  | 512√ó512 √ó 512√ó512      |   1293 ms    |
  | 1024√ó1024 √ó 1024√ó1024   |  10570 ms    |

  ---

  ## Strassen‚Äôs Algorithm ‚Äì ùëÇ(n^2.807)

  Strassen's method replaces some multiplications with additions, reducing the asymptotic complexity. This implementation:
  
  - Uses Strassen recursively.
  - Switches to naive multiplication at block size 64 for better locality and performance.
  
  | Matrix Size            | Time Elapsed |
  |------------------------|--------------|
  | 32√ó32 √ó 32√ó32          |      0 ms    |
  | 128√ó128 √ó 128√ó128      |      2 ms    |
  | 256√ó256 √ó 256√ó256      |     14 ms    |
  | 512√ó512 √ó 512√ó512      |     97 ms    |
  | 1024√ó1024 √ó 1024√ó1024   |    680 ms    |

  ---

  ## Summary

  We started with a 16-second naive algorithm for multiplying two 1024√ó1024 matrices and reduced it to under **700 milliseconds** by leveraging smarter recursion and cache-aware design ‚Äî without touching parallelism or SIMD.

  ---
